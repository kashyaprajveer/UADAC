{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1NmejTIFPtf"
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from envs import OfflineEnv\n",
    "from recommender import DRRAgent\n",
    "import torch\n",
    "\n",
    "STATE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df=pd.read_csv('./movies.dat')\n",
    "movies_list=movies_df.values.tolist()\n",
    "movies_id_to_movies = {movie[0]: movie[1:] for movie in movies_list}\n",
    "movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)\n",
    "ratings_df=pd.read_csv('./ratings.dat')\n",
    "ratings_df = ratings_df.applymap(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dict = np.load('./user_dict.npy', allow_pickle=True)\n",
    "users_history_lens = np.load('./users_histroy_len.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 유저별로 본 영화들 순서대로 정리\n",
    "users_dict = {user : [] for user in set(ratings_df[\"UserID\"])}\n",
    "users_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>255</td>\n",
       "      <td>4</td>\n",
       "      <td>874724710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>874724727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259</td>\n",
       "      <td>298</td>\n",
       "      <td>4</td>\n",
       "      <td>874724754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259</td>\n",
       "      <td>185</td>\n",
       "      <td>4</td>\n",
       "      <td>874724781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>173</td>\n",
       "      <td>4</td>\n",
       "      <td>874724843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0     259      255       4  874724710\n",
       "1     259      286       4  874724727\n",
       "2     259      298       4  874724754\n",
       "3     259      185       4  874724781\n",
       "4     259      173       4  874724843"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by time\n",
    "ratings_df = ratings_df.sort_values(by='Timestamp', ascending=True)\n",
    "ratings_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put (movie, rating) pairs in user dictionary\n",
    "# Only movies with a rating of 4 or higher are counted for each user's movie history length.\n",
    "ratings_df_gen = ratings_df.iterrows()\n",
    "users_dict_for_history_len = {user : [] for user in set(ratings_df[\"UserID\"])}\n",
    "for data in ratings_df_gen:\n",
    "    users_dict[data[1]['UserID']].append((data[1]['MovieID'], data[1]['Rating']))\n",
    "    if data[1]['Rating'] >= 1:\n",
    "        users_dict_for_history_len[data[1]['UserID']].append((data[1]['MovieID'], data[1]['Rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Movie history length for each user\n",
    "users_history_lens = [len(users_dict_for_history_len[u]) for u in set(ratings_df[\"UserID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_history_lens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save(\"./data/user_dict.npy\", users_dict)\n",
    "np.save(\"./data/users_histroy_len.npy\", users_history_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_num = max(ratings_df[\"UserID\"])+1\n",
    "items_num = max(ratings_df[\"MovieID\"])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 1683\n"
     ]
    }
   ],
   "source": [
    "print(users_num, items_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755 1683\n"
     ]
    }
   ],
   "source": [
    "train_users_num = int(users_num * 0.8)\n",
    "train_items_num = items_num\n",
    "print(train_users_num, train_items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755 755\n"
     ]
    }
   ],
   "source": [
    "train_users_dict = {k:users_dict[k] for k in range(1, train_users_num+1)}\n",
    "train_users_history_lens = users_history_lens[:train_users_num]\n",
    "print(len(train_users_dict),len(train_users_history_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 1683\n"
     ]
    }
   ],
   "source": [
    "eval_users_num = int(users_num * 0.2)\n",
    "eval_items_num = items_num\n",
    "print(eval_users_num, eval_items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 188\n"
     ]
    }
   ],
   "source": [
    "eval_users_dict = {k:users_dict[k] for k in range(users_num-eval_users_num, users_num)}\n",
    "eval_users_history_lens = users_history_lens[-eval_users_num:]\n",
    "print(len(eval_users_dict),len(eval_users_history_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(recommender, env, check_movies = False, top_k=False):\n",
    "\n",
    "\n",
    "\n",
    "   # episodic reward\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    mean_precision = 0\n",
    "    q_loss1 = 0\n",
    "    q_loss2 = 0\n",
    "    \n",
    "    # Environment\n",
    "    user_id, items_ids, done = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        # Observe current state & Find action\n",
    "        ## Embedding\n",
    "        user_eb = recommender.embedding_network.get_layer('user_embedding')(np.array(user_id))\n",
    "        items_eb = recommender.embedding_network.get_layer('movie_embedding')(np.array(items_ids))\n",
    "        ## SRM state\n",
    "        state = recommender.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(items_eb, axis=0)])\n",
    "        ## Action(ranking score)\n",
    "\n",
    "        action1 = recommender.actor.network(state)\n",
    "        action2 = recommender.actor2.network(state)\n",
    "\n",
    "        action = (action1 + action2)/2\n",
    "\n",
    "        recommended_item = recommender.recommend_item(action, recommender.env.recommended_items, top_k=top_k)\n",
    "\n",
    "        next_items_ids, reward, done, _ = recommender.env.step(recommended_item, top_k=top_k)\n",
    "        if top_k:\n",
    "            reward = np.sum(reward)\n",
    "\n",
    "        # get next_state\n",
    "        next_items_eb = recommender.embedding_network.get_layer('movie_embedding')(np.array(next_items_ids))\n",
    "        next_state = recommender.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(next_items_eb, axis=0)]) \n",
    "\n",
    "        recommender.buffer.append(state, action, reward, next_state, done)\n",
    "\n",
    "        if recommender.buffer.crt_idx > 1 or recommender.buffer.is_full:\n",
    "            recommender.itr += 1\n",
    "\n",
    "            # Sample a minibatch of batch_size=32\n",
    "            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones, weight_batch, index_batch = recommender.buffer.sample(recommender.batch_size)\n",
    "\n",
    "\n",
    "            # get actions with noise\n",
    "\n",
    "            noise = torch.FloatTensor(recommender.target_policy_noise.sample())\n",
    "            clipped_noise = torch.clamp(noise, -recommender.target_policy_noise_clip, recommender.target_policy_noise_clip)\n",
    "\n",
    "            target_next_action =recommender.actor.target_network(batch_next_states) + clipped_noise\n",
    "\n",
    "\n",
    "\n",
    "            target_qs = recommender.critic.target_network([target_next_action, batch_next_states])\n",
    "            target_qs2 = recommender.critic2.target_network([target_next_action, batch_next_states])\n",
    "\n",
    "            min_qs = tf.raw_ops.Min(input=tf.concat([target_qs, target_qs2], axis=1), axis=1, keep_dims=True) \n",
    "\n",
    "            td_targets = recommender.calculate_td_target(batch_rewards, min_qs, batch_dones)\n",
    "\n",
    "            for (p, i) in zip(td_targets, index_batch):\n",
    "                recommender.buffer.update_priority(abs(p[0]) + recommender.epsilon_for_priority, i)\n",
    "\n",
    "\n",
    "\n",
    "            q_loss1 += recommender.critic.train([batch_actions, batch_states], td_targets, weight_batch)\n",
    "            q_loss2 += recommender.critic2.train([batch_actions, batch_states], td_targets, weight_batch)\n",
    "\n",
    "\n",
    "\n",
    "            if recommender.itr % 5 == 0:\n",
    "                s_grads1 = recommender.critic.dq_da([recommender.actor.network(batch_states), batch_states]) # returns q_grads\n",
    "                s_grads2 = recommender.critic2.dq_da([recommender.actor2.network(batch_states), batch_states])\n",
    "\n",
    "                #s_grads = (s_grads1 + s_grads2)\n",
    "\n",
    "                recommender.actor.train(batch_states, s_grads1)\n",
    "                recommender.actor2.train(batch_states, s_grads2)\n",
    "\n",
    "                #Update TARGET networks line 17 of paper\n",
    "                recommender.actor.update_target_network()  # Using the actor network\n",
    "                recommender.actor2.update_target_network()\n",
    "                recommender.critic.update_target_network() # Using the critic network\n",
    "                recommender.critic2.update_target_network()\n",
    "\n",
    "        items_ids = next_items_ids\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    return episode_reward\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 6500, reward:-160.5\n",
      "Model: 7000, reward:-369.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_reward = 0\n",
    "for i in range(1,20):\n",
    "\n",
    "    sum_reward = 0\n",
    "#     TOP_K = 5\n",
    "    for user_id in eval_users_dict.keys():\n",
    "        env = OfflineEnv(eval_users_dict, users_history_lens, movies_id_to_movies, STATE_SIZE, fix_user_id=user_id)\n",
    "        recommender = DRRAgent(env, users_num, items_num, STATE_SIZE)\n",
    "        recommender.actor.build_networks()\n",
    "        recommender.actor2.build_networks()\n",
    "        recommender.critic.build_networks()\n",
    "        recommender.critic2.build_networks()\n",
    "        recommender.load_model(f\"./save_weights/actor_{i*500}_fixed.h5\", \n",
    "                               f\"./save_weights/actor2_{i*500}_fixed.h5\",\n",
    "                               f\"./save_weights/critic_{i*500}_fixed.h5\",\n",
    "                              f\"./save_weights/critic2_{i*500}_fixed.h5\")\n",
    "        reward = evaluate(recommender, env, top_k=False)\n",
    "\n",
    "        sum_reward += reward\n",
    "\n",
    "    print(f'Model: {i*500}, reward:{sum_reward}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model: 6000, reward:-66.5\n",
    "Model: 6500, reward:-160.5\n",
    "Model: 7000, reward:-369.5"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "RL_ActorCritic_DDPG_Movie_Recommendation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
